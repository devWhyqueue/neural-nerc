\section{Dataset and Preprocessing}
We use the \textbf{DDIExtraction 2013 corpus} for training and evaluating the NER model. The dataset contains documents drawn from two sources: DrugBank (comprehensive drug descriptions) and MedLine abstracts (scientific literature). Each document is annotated with drug name entities of four types: \textit{drug} (generic drug names), \textit{brand} (brand names), \textit{group} (drug categories/classes), and \textit{drug\_n} (drug names that are combos or not approved for human use). These annotations are provided in XML format with character offsets for each entity mention. We focus on the NERC subtask (Task 9.1) of recognizing and classifying these pharmacological substances~\cite{semeval2013}, casting it as a sequence labeling problem.

The data is split into training, development (validation), and test sets as specified by the challenge. In total, the corpus includes over 5,000 sentences for training, and roughly 1,400 sentences each for development and test. Each sentence is tokenized and each token is labeled using the standard BIO scheme: \textbf{B-}$X$ denotes the beginning of an entity of type $X$, \textbf{I-}$X$ denotes a continuation of an entity of type $X$, and \textbf{O} denotes a token that is not part of any named entity. For example:

\begin{quote}
\small
``We present a case of a 23-year-old man on drug therapy with \underline{rifampicin} and \underline{isoniazid}.'' 
\end{quote}

Here \emph{rifampicin} and \emph{isoniazid} are drug names and would be tagged as B-drug (if single-token entities) or B-drug/I-drug if multi-token (though in this case each is one token). All other words receive the O tag.

We used the provided \texttt{Dataset} loader (from the given code) to parse the XML files and produce token-level annotations. This loader handles sentence extraction, applies tokenization (using a word tokenizer that splits punctuation from words), and assigns BIO labels to each token by checking if its character span matches any annotated entity. Notably, the code merges discontinuous annotations by considering only the first span, but such cases are very rare in the data. We did not modify this data loading procedure, in accordance with the task instructions, to ensure consistency in evaluation.

After tokenization and labeling, we perform indexing of words and labels. All unique token forms in the training set are added to a vocabulary, and each is assigned an integer ID. Likewise, each distinct suffix of length up to $L=5$ characters is added to a suffix vocabulary. Suffixes (e.g., ``\textit{-cin}'' from ``rifampicin'') can provide orthographic clues; many pharmaceutical terms have common suffix patterns (like \textit{-azole}, \textit{-amine}) that indicate certain drug categories. By including suffixes as features, the model can generalize to unseen drug names based on their morphological endings. Each entity label (B/I for each type, and O) is also assigned an index. A special ID 0 is reserved for padding tokens and an ID 1 for unknown words/suffixes. The maximum sentence length was set to 150 tokens (long enough to cover the longest sentence in the data), and any shorter sentence is padded to this length (longer ones are truncated, though none in the training set exceeded 150). This indexing and padding is handled by the provided \texttt{Codemaps} class. We did not alter the indexing code; we only adjusted the maximum length and suffix length parameters as needed (these were permitted parameters to change). In our case, we used the default max length of 150 and suffix length of 5 given in the instructions. The output of preprocessing is a set of numerical matrices: for each sentence, a sequence of word indices and a parallel sequence of suffix indices (both length 150 with padding), and a sequence of label indices for the true tags. 
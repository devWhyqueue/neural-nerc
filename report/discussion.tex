\section{Discussion}
Our results confirm that a neural network can effectively learn to recognize and classify drug-related entities from biomedical text with minimal feature engineering. Compared to the baseline, which required manual feature design (e.g., patterns for capitalization, suffix lists, etc.), the BiLSTM model automatically learns representations that capture these signals. For example, the embedding for a token like ``acetaminophen'' likely learns that it is similar to other drug embeddings, and the LSTM context can disambiguate it from cases where the same word might not be a drug (though in this domain, that ambiguity is rare).

One notable aspect is the use of suffix embeddings. This is a simple way to incorporate sub-word information. An alternative approach could be to use a character-level LSTM or convolution to learn features for each word from its characters. That might capture prefixes and infixes as well, whereas our suffix approach only looks at the last $n$ characters. However, the suffix method proved adequate for this task, since many discriminative cues are indeed at the ends of drug names (due to naming conventions in pharmacology).

We also explored the impact of using \textbf{pretrained word embeddings}. Although not provided in the initial code, we extended the model to initialize the word embedding matrix with 200-dimensional embeddings from the PubMed domain (trained on biomedical texts). This change was within allowed modifications as per the task description. In preliminary trials, using these pretrained embeddings gave a slight boost of about +1.5 F$_1$ on the dev set, mainly by improving recall on rare entities. We ultimately reported the simpler model for clarity, but this suggests a clear avenue for enhancement: leveraging external unlabeled data via pretrained embeddings can improve the NER performance further.

Another possible enhancement is adding a second BiLSTM layer (stacked RNN) to capture higher-level features, or using a CRF decoding layer to jointly decode the sequence with valid BIO constraints. Both are common in modern NER systems. We did not implement the CRF due to the project constraint of focusing on certain code areas, but it would likely improve precision by eliminating some boundary errors as mentioned.

Error analysis indicates that distinguishing entity \emph{types} is harder than just recognizing an entity boundary. Our model's confusion between, say, \texttt{Drug} vs \texttt{Group} could be addressed by incorporating global context of the document or using an ontology. However, given the scope of this project, we restricted ourselves to sentence-level predictions.

In terms of code, we followed the instruction to modify only the neural network construction and associated parameters. The \texttt{train.py} script was updated to build the model as described and to train it, while the \texttt{predict.py} script (used for running the model on new data and formatting output) remained mostly unchanged except for ensuring it loads the correct model and index files. These modifications are documented in the code listing attached. The clear separation of data loading, model building, and evaluation in the provided code structure made it easy to plug in our new model without altering other components. 